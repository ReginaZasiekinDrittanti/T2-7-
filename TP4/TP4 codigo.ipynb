{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jbQnrDd5kPBV"
   },
   "source": [
    "# Trabajo Práctico 4\n",
    "Encuesta Permanente de Hogares (EPH) \n",
    "\n",
    "Integrantes: Regina Zasiekin Drittanti, Mila Palacios Vieira y Milagros Arjona"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de Base TP 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc, RocCurveDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_04_inicial = pd.read_stata('/Users/usuario/Downloads/Individual_t104(1).dta')  \n",
    "base_24_inicial = pd.read_excel('/Users/usuario/Downloads/usu_individual_T124(3).xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminen todas las observaciones que no corresponden a los aglomerados de Ciudad Autónoma de Buenos Aires o Gran Buenos Aires, y unan ambos trimestres en una sola base. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# me quedo solo con los valores de CABA y GBA\n",
    "base_04_filtrada = base_04_inicial.loc[base_04_inicial['aglomerado'].isin(['Ciudad de Buenos Aires', 'Partidos del GBA'])]\n",
    "base_24_filtrada = base_24_inicial.loc[base_24_inicial['AGLOMERADO'].isin([32, 33])]\n",
    "\n",
    "#Para poder unir sin problema, me aseguro que als variables esten en el mismo formato\n",
    "base_04_filtrada.columns = base_04_filtrada.columns.str.lower()\n",
    "base_24_filtrada.columns = base_24_filtrada.columns.str.lower()\n",
    "\n",
    "# concateno las bases\n",
    "union_eph = pd.concat([base_24_filtrada, base_04_filtrada])\n",
    "print(union_eph.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# como hay datos que tienen distintas etiquetas en las dos bases que concatenamos, tenemos que unificar renombrar las etiquetas de una de las bases asi se pueden tener todos los datos con el mismo valor\n",
    "variables_interes = union_eph[[\"ch04\", \"ch06\", \"ch07\", \"ch08\", \"nivel_ed\", \"estado\", \"cat_inac\"]]\n",
    "\n",
    "for i in variables_interes:\n",
    "    if i == \"ch04\": \n",
    "        union_eph['ch04'] = union_eph['ch04'].replace({\n",
    "            'Mujer': 2,\n",
    "            'Varón': 1})\n",
    "    elif i == \"ch06\":\n",
    "        union_eph['ch06'] = union_eph['ch06'].replace({\n",
    "            '98 y más años' : 98, \n",
    "            'Menos de 1 año' : 0})\n",
    "        # cambio el formato de los valores de esta columna porque la mitad estaba en float y la mitas en int\n",
    "        union_eph['ch06'] = union_eph['ch06'].astype(int)\n",
    "    elif i == \"ch08\":\n",
    "        union_eph['ch08'] = union_eph['ch08'].replace({\n",
    "            'Obra social (incluye PAMI)': 1,  \n",
    "            'Mutual/Prepaga/Servicio de emergencia': 2,  \n",
    "            'Planes y seguros públicos': 3,  \n",
    "            'No paga ni le descuentan': 4,  \n",
    "            'Ns./Nr.': 9,  \n",
    "            'Obra social y mutual/prepaga/servicio de emergencia': 12,  \n",
    "            'Obra social y planes y seguros públicos': 13,  \n",
    "            'Mutual/prepaga/servicio de emergencia/planes y seguros públi': 23,\n",
    "            'Obra Social, mutual/prepaga/servicio de emergencia y planes y seguros públicos': 123})\n",
    "    elif i == \"nivel_ed\":\n",
    "        union_eph['nivel_ed'] = union_eph['nivel_ed'].replace({\n",
    "            1: 'Primaria Incompleta (incluye educación especial)',  \n",
    "            2: 'Primaria Completa',  \n",
    "            3: 'Secundaria Incompleta',  \n",
    "            4: 'Secundaria Completa',  \n",
    "            5: 'Superior Universitaria Incompleta',  \n",
    "            6: 'Superior Universitaria Completa', \n",
    "            7: 'Sin instrucción',  \n",
    "            9: 'Ns./ Nr. '})\n",
    "\n",
    "    conteo = union_eph.groupby(i).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario para mapear los valores originales de ch04 (Género) con nombres descriptivos\n",
    "map_ch04 = {\n",
    "    1: 'Masculino',\n",
    "    2: 'Femenino'\n",
    "}\n",
    "\n",
    "\n",
    "map_ch07 = {\n",
    "    1: 'Unido',\n",
    "    2: 'Casado',\n",
    "    3: 'Separado o divorciado',\n",
    "    4: 'Viudo',\n",
    "    5: 'Soltero'\n",
    "}\n",
    "\n",
    "map_estado = {\n",
    "    1: 'Ocupado',\n",
    "    2: 'Desocupado',\n",
    "    3: 'Inactivo',\n",
    "    4: 'Menor de 10 años',\n",
    "    0: 'Entrevista individual no realizada (no respuesta al cuestion'\n",
    "}\n",
    "\n",
    "map_cat_inac = {\n",
    "    1: 'Jubilado/pensionado',\n",
    "    2: 'Rentista',\n",
    "    3: 'Estudiante',\n",
    "    4: 'Ama de casa',\n",
    "    5: 'Menor de 6 años',\n",
    "    6: 'Discapacitado',\n",
    "    7: 'Otros'\n",
    "}\n",
    "\n",
    "# Reemplazar valores en las columnas antes de generar dummies\n",
    "union_eph['ch04'] = union_eph['ch04'].replace(map_ch04)\n",
    "union_eph['ch07'] = union_eph['ch07'].replace(map_ch07)\n",
    "union_eph['estado'] = union_eph['estado'].replace(map_estado)\n",
    "union_eph['cat_inac'] = union_eph['cat_inac'].replace(map_cat_inac)\n",
    "\n",
    "# Crear variables dummies con nombres descriptivos\n",
    "union_eph = pd.get_dummies(union_eph, columns=['ch04','ch07', 'estado', 'cat_inac'], drop_first=False)\n",
    "\n",
    "# Mostrar las primeras filas para verificar los nombres descriptivos\n",
    "print(\"Variables dummies con nombres descriptivos:\")\n",
    "print(union_eph.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar las columnas dummies recién creadas\n",
    "dummies_cols = [col for col in union_eph.columns if 'ch07_' in col or 'estado_' in col or 'cat_inac_' in col]\n",
    "\n",
    "# Convertir las dummies de booleanos a enteros\n",
    "union_eph[dummies_cols] = union_eph[dummies_cols].astype(int)\n",
    "\n",
    "# Verificar que las dummies están en formato 1 y 0\n",
    "print(union_eph[dummies_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observaciones con valores que no tienen sentido, descártenlas (por ejemplo, ingresos y edades negativos). Expliquen las decisiones tomadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificación de observaciones negativas en EDAD\n",
    "\n",
    "valores_negativos_edad = union_eph['ch06'] < 0\n",
    "# cantidad de datos negativos\n",
    "cantidad_negativos_edad = valores_negativos_edad.sum()\n",
    "print(\"Cantidad de valores negativos en variable edad:\", cantidad_negativos_edad)\n",
    "\n",
    "# me quedo con los valores mayores o iguales a 0 de edad\n",
    "base_limpia = union_eph[union_eph['ch06'] >= 0]\n",
    "\n",
    "# Verificación de observaciones negativas en IPCF\n",
    "valores_negativos_ipcf = union_eph[union_eph['ipcf'] < 0]\n",
    "# cantidad de datos negativos\n",
    "cantidad_negativos_ipcf = len(valores_negativos_ipcf)\n",
    "# Mostrar resultados\n",
    "print(\"Cantidad de valores negativos en variable ipcf:\", cantidad_negativos_ipcf)\n",
    "\n",
    "# me quedo con los valores mayores o iguales a 0. Como no hay valores negativos, no saca ninguno\n",
    "base_limpia = union_eph[union_eph['ipcf'] >= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Exploren el diseño de registro de la base de hogar: a priori, ¿qué variables creen pueden ser predictivas de la desocupación y seria útil\n",
    "incluir para perfeccionar el ejercicio del TP3? Mencionen estas variables y justifiquen su elección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se respondió en el informe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Descarguen la base de microdatos de la EPH correspondiente al primer trimestre de 2004 y 2024 en formato .dta y .xls, respectivamente. La\n",
    "base de hogares se llama Hogar_t104.dta y usu_hogar_T124.xls, respectivamente. Eliminen todas las observaciones que no corresponden a los aglomerados de Ciudad Autónoma de Buenos Aires o Gran Buenos Aires y unan ambos trimestres en una sola base. Esto es, a la base de la encuesta individual de cada año (que usaron en el TP3) unan la base de la encuesta de hogar. Asegúrese de estar usando las variables CODUSU y NRO_Hogar para el merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Limpien la base de datos tomando criterios que hagan sentido. Explicar cualquier decisión como el tratamiento de valores faltantes (missing\n",
    "values), extremos (outliers), o variables categóricas. Justifique sus decisiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Construya variables (mínimo 3) que no estén en la base pero que sean relevantes para predecir individuos desocupados (por ejemplo, la\n",
    "proporción de personas que trabajan en el hogar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Presenten estadísticas descriptivas de tres variables de la encuesta de hogar que ustedes creen que pueden ser relevantes para predecir la\n",
    "desocupación. Comenten las estadísticas obtenidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte II: Clasificación y regularización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Para cada año, partan la base respondieron en una base de prueba y una de entrenamiento (X_train, y_train, X_test, y_test) utilizando\n",
    "el comando train_test_split. La base de entrenamiento debe comprender el 70% de los datos, y la semilla a utilizar (random state\n",
    "instance) debe ser 101. Establezca a desocupado como su variable dependiente en la base de entrenamiento (vector y). El resto de las\n",
    "variables serán las variables independientes (matriz X). Recuerden agregar la columna de unos (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Expliquen brevemente cómo elegirían λ por validación cruzada (en Python es alpha). Detallen por qué no usarían el conjunto de prueba\n",
    "(test) para su elección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. En validación cruzada, ¿cuáles son las implicancias de usar un k muy pequeño o uno muy grande? Cuando k = n (con n el número de\n",
    "muestras), ¿cuántas veces se estima el modelo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Para regresión logística, implementen la penalidad, L1 como la de LASSO y L2 como la de Ridge con λ = 1 (como en la Tutorial 10), usando\n",
    "la opción penalty y reporten la matriz de confusión, la curva ROC, los valores de AUC y de Accuracy para cada año.1 ¿Cómo cambiaron los\n",
    "resultados con respecto al TP3? ¿La performance de regresión logística con regularización es mejor o peor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `modelo` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Realicen un barrido en = 10n λ con n ∈ {−5, −4, −3 ..., +4, +5} y utilicen 10-fold CV para elegir el λ óptimo en regresión logística con Ridge y con\n",
    "LASSO. ¿Qué λ seleccionó en cada caso? Usando la librería de seaborn, generen box plot mostrando la distribución del error de predicción para\n",
    "cada λ. Cada box debe corresponder a un valor de λ y contener como observaciones el error medio de validación (MSE) para cada partición.\n",
    "Además, para la regularización LASSO, generen un line plot del promedio de la proporción de variables ignoradas por el modelo en\n",
    "función de λ (como vieron en el tutorial 10), es decir la proporción de variables para las cuales el coeficiente asociado es cero.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. En el caso del valor óptimo de λ para LASSO encontrado en el inciso anterior, ¿qué variables fueron descartadas? ¿Son las que hubieran\n",
    "esperado? ¿Tiene relación con lo que respondieron en el inciso 1 de la Parte I?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Elijan alguno de los modelos de regresión logística donde hayan probado distintos parámetros de regularización y comenten: Compare los\n",
    "resultados de 2004 versus 2024, ¿qué método de regularización funcionó mejor: Ridge o LASSO? ¿LASSO hizo una selección distinta de\n",
    "predictores en 2004 versus 2024? Comenten mencionando el error cuadrático medio (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Datos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
